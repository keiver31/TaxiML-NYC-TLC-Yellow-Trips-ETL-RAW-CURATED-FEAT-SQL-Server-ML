# `validate_model.py` ‚Äî Validaci√≥n del modelo (Regresi√≥n: `trips_count`)

Este documento describe la **funcionalidad** del script `validate_model.py` para que cualquier persona (t√©cnica o no t√©cnica) pueda:
- entender qu√© valida,
- saber c√≥mo ejecutarlo,
- interpretar sus m√©tricas y salidas,
- reutilizarlo en su propio proyecto.

---

## Contexto: ¬øqu√© est√° prediciendo el modelo?

El objetivo del modelo es predecir **cu√°ntos viajes** (`trips_count`) ocurren en una combinaci√≥n espec√≠fica (por ejemplo: zona + hora).

Durante el entrenamiento, el modelo **no** aprendi√≥ a predecir `trips_count` directamente, sino una versi√≥n transformada:

- Transformaci√≥n usada:  
  `y_log = log(1 + trips_count)` (en Python: `np.log1p(trips_count)`)

**¬øPor qu√©?**  
En datos de conteo suele haber muchos valores bajos (incluyendo ceros) y pocos ‚Äúpicos‚Äù muy altos. La transformaci√≥n `log(1 + y)` reduce esa asimetr√≠a y hace el aprendizaje m√°s estable.

---

## Qu√© hace este script (resumen)

1. Carga artefactos: modelo + `X_test` + `y_test_real`.
2. Predice en **escala log** (salida del modelo).
3. Convierte a **escala real** (conteos) con la inversa de la transformaci√≥n.
4. Calcula m√©tricas principales (MAE, RMSE, R¬≤) en escala real.
5. Construye un **baseline** simple (predice la mediana) y calcula mejoras porcentuales.
6. Calcula percentiles del error (P50, P90, P95) para ver la ‚Äúcola‚Äù de errores.
7. Emite un **dictamen** (heur√≠stico) basado en mejora vs baseline y una se√±al por R¬≤.
8. Eval√∫a desempe√±o por rangos de conteo (bajo/medio/alto/pico).
9. Guarda m√©tricas e informaci√≥n por fila para comparar iteraciones y diagnosticar.

---

## Entradas esperadas

El script espera encontrar estos archivos en la carpeta `artifacts/`:

| Archivo | Qu√© contiene | Notas |
|---|---|---|
| `linreg_trips_count_v2.joblib` | Modelo entrenado (scikit-learn) | Se carga con `joblib.load()` |
| `X_test_v2.csv` | Features del set de prueba | Debe tener **las mismas columnas** usadas al entrenar |
| `y_test_real_v2.csv` | `trips_count` real del set de prueba | **Conteos reales**, no log |

**Requisito cr√≠tico**  
`X_test_v2.csv` debe coincidir con el set de features del entrenamiento (mismas columnas y orden). Si no, `model.predict(X_test)` puede fallar o producir resultados incorrectos.

---

## Salidas generadas

| Archivo | Qu√© contiene | Para qu√© sirve |
|---|---|---|
| `artifacts/metrics_summary.txt` | L√≠nea por ejecuci√≥n con m√©tricas + baseline + percentiles | Comparar iteraciones (v1/v2/v3‚Ä¶) |
| `artifacts/validation_results.csv` | Resultado por fila: `y_real`, `y_pred`, `abs_error` | Auditor√≠a / diagn√≥stico / an√°lisis posterior |

Adem√°s, imprime un reporte completo en consola y muestra los **primeros 5 ejemplos**.

---

## Requisitos para ejecutar

Dependencias Python:
- `pandas`
- `numpy`
- `joblib`
- `scikit-learn`

Instalaci√≥n:
```bash
pip install pandas numpy joblib scikit-learn
```

Ejecuci√≥n:
```bash
python validate_model.py
```

---

## C√≥mo funciona: secci√≥n por secci√≥n (mapeado al c√≥digo)

### 1) Cargar modelo y datos de test

- Carga el modelo entrenado desde `.joblib`
- Lee `X_test` (matriz de variables de entrada)
- Lee `y_test` real (vector de conteos)

Detalle pr√°ctico:
- `squeeze("columns")` convierte un CSV de una sola columna a un vector 1D (Serie).

Validaci√≥n r√°pida que imprime el script:
- `X_test.shape` (n_filas, n_features)
- `y_test.shape` (n_filas)

**Regla simple:** `X_test` y `y_test` deben tener el **mismo n√∫mero de filas**.

---

### 2) Predicci√≥n en escala LOG y devoluci√≥n a escala REAL

El modelo devuelve:
- `pred_log ‚âà log(1 + trips_count)`

Para volver a conteo real:
- Si `pred_log = log(1 + y)`, entonces `y = exp(pred_log) - 1`

En el script:
- `pred = np.expm1(pred_log)`  (equivalente a `np.exp(pred_log) - 1`, pero m√°s estable)

Luego se aplica una protecci√≥n:
- `pred = np.clip(pred, 0, None)`

**¬øPor qu√©?**  
Un conteo real no deber√≠a ser negativo. Por ruido del modelo, la inversi√≥n puede dar valores menores a 0; el `clip` fuerza m√≠nimo 0.

---

### 3) M√©tricas principales (en escala REAL)

Se calculan comparando `y_test` (real) vs `pred` (real):

- **MAE (Mean Absolute Error)**  
  Promedio de `|y - y_pred|`  
  Interpretaci√≥n: ‚Äúen promedio, me equivoco por X viajes‚Äù.

- **RMSE (Root Mean Squared Error)**  
  Ra√≠z del error cuadr√°tico medio  
  Interpretaci√≥n: penaliza m√°s los errores grandes (picos).

- **R¬≤ (Coeficiente de determinaci√≥n)**  
  Interpretaci√≥n: qu√© tan bien el modelo explica la variaci√≥n del objetivo.  
  Valores t√≠picos:
  - `1.0` = perfecto
  - cercano a `0` = similar a un baseline simple
  - negativo = peor que un baseline (depende del caso)

---

### 4) Baseline (modelo ‚Äútonto‚Äù) usando la mediana

Se construye un baseline que predice **la misma constante para todas las filas**:
- `baseline_pred = mediana(y_test)`

**Motivo:**  
En conteos con distribuci√≥n sesgada, la mediana suele ser un baseline robusto.

Se calculan:
- `baseline_mae`, `baseline_rmse`

Y luego la mejora porcentual:
- `ImproveMAE% = (baseline_mae - mae) / baseline_mae * 100`
- `ImproveRMSE% = (baseline_rmse - rmse) / baseline_rmse * 100`

Interpretaci√≥n:
- mejora **positiva** => el modelo supera al baseline
- mejora **‚â§ 0** => el modelo no aporta mejora real vs ‚Äúpredecir la mediana‚Äù

---

### 5) Distribuci√≥n de errores (percentiles)

Se calcula error absoluto por fila:
- `abs_error = |y_real - y_pred|`

Y percentiles:
- **P50**: error ‚Äút√≠pico‚Äù (mediana del error)
- **P90** / **P95**: cola del error (casos dif√≠ciles, picos, outliers)

Esto ayuda a responder:
- ‚Äú¬øQu√© tan grande es el error en el peor 10% o 5% de casos?‚Äù

---

### 6) Dictamen (heur√≠stico)

El script genera un dictamen para ayudar a decidir si el modelo ‚Äúpasa‚Äù o ‚Äúno pasa‚Äù con reglas simples:

1) Regla base:
- si **no mejora** MAE vs baseline ‚Üí **üî¥ Todav√≠a NO**

2) Si mejora MAE vs baseline, clasifica con umbrales:

- ‚úÖ **Bien entrenado** si:
  - `ImproveMAE% >= 30` **y** `ImproveRMSE% >= 35`

- üü° **Aceptable** si:
  - `ImproveMAE% >= 15` **y** `ImproveRMSE% >= 20`

- üî¥ **Todav√≠a NO** si:
  - la mejora existe, pero es d√©bil

Se a√±ade una se√±al extra:
- si `R¬≤ < 0.2` agrega raz√≥n: ‚ÄúR2 bajo‚Äù.

> Nota: estos umbrales son **criterios pr√°cticos** del proyecto (reglas internas). Si cambias dataset o contexto, aj√∫stalos.

---

### 7) Reporte en consola

Imprime:
- M√©tricas principales (MAE/RMSE/R¬≤)
- Media y mediana del objetivo real
- M√©tricas del baseline + mejoras porcentuales
- Percentiles del error
- Dictamen + razones

---

### 7.5) Evaluaci√≥n por rangos (BAJO / MEDIO / ALTO / PICO)

Prop√≥sito:
- ver c√≥mo cambia el error cuando el conteo real es peque√±o vs grande

El script define segmentos por `trips_count` real:
- **BAJO:** `<= 20`
- **MEDIO:** `20‚Äì200`
- **ALTO:** `> 200`
- **PICO:** `> 500`

Para cada segmento calcula:
- `n` (n√∫mero de filas)
- MAE, RMSE
- `y_mean` del segmento

---

### 7.6) Guardar resumen de m√©tricas (historial)

Se escribe en modo append (`"a"`) en:
- `artifacts/metrics_summary.txt`

Cada ejecuci√≥n agrega una l√≠nea con:
- MAE, RMSE, R¬≤
- baseline MAE/RMSE
- mejoras %
- P50/P90/P95

Esto permite comparar f√°cilmente resultados entre versiones del modelo.

---

### 8) Export de resultados por fila

Se exporta el CSV completo:
- `artifacts/validation_results.csv`

Columnas:
- `y_real`: valor real
- `y_pred`: predicci√≥n (conteo)
- `abs_error`: error absoluto por fila

Tambi√©n se imprime una muestra con los primeros 5 registros para inspecci√≥n r√°pida.

---

## Personalizaci√≥n r√°pida

Si cambias versi√≥n de artefactos (v3, v4...), actualiza estos nombres:
- `linreg_trips_count_v2.joblib`
- `X_test_v2.csv`
- `y_test_real_v2.csv`

Si tu distribuci√≥n es distinta, ajusta los segmentos:
- BAJO / MEDIO / ALTO / PICO

Si quieres conteos enteros, podr√≠as redondear `pred`, pero **ten en cuenta** que cambia m√©tricas y an√°lisis:
- `pred = np.rint(pred)` (opcional y depende del caso)

---

## Troubleshooting (fallos t√≠picos)

**1) `FileNotFoundError`**  
- Los archivos no est√°n en `artifacts/` o el nombre no coincide.

**2) Error en `model.predict(X_test)`**  
- `X_test` no tiene las mismas columnas que el entrenamiento.  
  Recomendaci√≥n: reconstruir `X_test` usando el mismo pipeline de features.

**3) Predicciones negativas**  
- Es esperable por la inversi√≥n `expm1` + ruido del modelo.  
  El script ya lo corrige con `np.clip(pred, 0, None)`.

**4) NaNs o tipos raros**  
- Revisa `X_test` por valores faltantes y tipos (`X_test.isna().sum()`).

---

## Glosario

- **Artefactos:** archivos generados por el pipeline (modelo, datasets, m√©tricas).
- **Feature:** variable de entrada del modelo (columna de `X_test`).
- **Objetivo / target:** variable a predecir (`trips_count`).
- **`log1p`:** `log(1 + y)` (estable con ceros).
- **`expm1`:** `exp(x) - 1` (inversa de `log1p`).
- **Baseline:** referencia simple para comparar (aqu√≠: mediana constante).
- **Percentil:** valor bajo el cual cae un % de datos (P90 = 90% por debajo).
- **M√°scara booleana (mask):** vector `True/False` para filtrar filas por condici√≥n.
- **MAE:** error absoluto promedio.
- **RMSE:** penaliza m√°s los errores grandes.
- **R¬≤:** capacidad explicativa global del modelo.

